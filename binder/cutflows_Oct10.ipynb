{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the LateX cutflow tables\n",
    "- Will load the pkl files that contain the cutflows and the sumgenweight\n",
    "- Will scale the events by the cross section\n",
    "- Will save the yields in a dictionnary called ```cutflows -> Dict()```\n",
    "- Will make the LateX table using the function ```make_composition_table()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "import hist as hist2\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import yaml\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "sys.path\n",
    "sys.path.append(\"../python/\")\n",
    "\n",
    "import utils\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({\"font.size\": 20})\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from scipy.special import softmax\n",
    "\n",
    "import hist as hist2\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "\n",
    "import sys\n",
    "sys.path\n",
    "sys.path.append(\"../python/\")\n",
    "\n",
    "import utils\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ele': {'Run2': 137640.0,\n",
       "  '2016APV': 19492.72,\n",
       "  '2016': 16809.96,\n",
       "  '2017': 41476.02,\n",
       "  '2018': 59816.23},\n",
       " 'mu': {'Run2': 137640.0,\n",
       "  '2016APV': 19436.16,\n",
       "  '2016': 16810.81,\n",
       "  '2017': 41475.26,\n",
       "  '2018': 59781.96},\n",
       " 'lep': {'Run2': 137640.0,\n",
       "  '2016APV': 19436.16,\n",
       "  '2016': 16810.81,\n",
       "  '2017': 41475.26,\n",
       "  '2018': 59781.96},\n",
       " 'had': {'Run2': 137640.0,\n",
       "  '2016APV': 19436.16,\n",
       "  '2016': 16810.81,\n",
       "  '2017': 41475.26,\n",
       "  '2018': 59781.96}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get lumi\n",
    "with open(\"../fileset/luminosity.json\") as f:\n",
    "    luminosity = json.load(f)\n",
    "    \n",
    "luminosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lumi(years, channels):\n",
    "    lum_ = 0\n",
    "    for year in years:\n",
    "        lum = 0\n",
    "        for ch in channels:\n",
    "            lum += luminosity[ch][year] / 1000.0\n",
    "\n",
    "        lum_ += lum / len(channels)    \n",
    "    return lum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read cutflows from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sum_sumgenweight(pkl_files, year, sample):\n",
    "    \"\"\"Load and sum the sumgenweight of each pkl file.\"\"\"\n",
    "    \n",
    "    sum_sumgenweight = 0\n",
    "    for ifile in pkl_files:\n",
    "        with open(ifile, \"rb\") as f:\n",
    "            metadata = pkl.load(f)            \n",
    "        sum_sumgenweight = sum_sumgenweight + metadata[sample][year][\"sumgenweight\"]\n",
    "\n",
    "    return sum_sumgenweight\n",
    "\n",
    "\n",
    "def get_xsecweight(pkl_files, year, ch, sample, is_data):\n",
    "    \n",
    "    if not is_data:\n",
    "        # find xsection\n",
    "        f = open(\"../fileset/xsec_pfnano.json\")\n",
    "        xsec = json.load(f)\n",
    "        f.close()\n",
    "        try:\n",
    "            xsec = eval(str((xsec[sample])))\n",
    "        except ValueError:\n",
    "            print(f\"sample {sample} doesn't have xsecs defined in xsec_pfnano.json so will skip it\")\n",
    "            return None\n",
    "\n",
    "        # get overall weighting of events.. each event has a genweight...\n",
    "        # sumgenweight sums over events in a chunk... sum_sumgenweight sums over chunks\n",
    "        xsec_weight = (xsec * luminosity[ch][year]) / get_sum_sumgenweight(pkl_files, year, sample)\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "    return xsec_weight\n",
    "\n",
    "def get_cutflow(pkl_files, year, ch, sample, is_data):\n",
    "    \"\"\"\n",
    "    Get cutflow from metadata but multiply by xsec-weight\n",
    "    \"\"\"\n",
    "    xsec_weight = get_xsecweight(pkl_files, year, ch, sample, is_data)\n",
    "        \n",
    "    cuts = [\n",
    "        \"sumgenweight\",\n",
    "        \"Trigger\",\n",
    "        \"METFilters\",\n",
    "        \"OneLep\",\n",
    "        \"NoTaus\",\n",
    "        \"AtLeastOneFatJet\",\n",
    "        \"CandidateJetpT\",\n",
    "        \"LepInJet\",\n",
    "        \"JetLepOverlap\",\n",
    "        \"dPhiJetMET\",\n",
    "        \"MET\",\n",
    "    ]\n",
    "        \n",
    "    if year == \"2018\":\n",
    "        cuts += [\"HEMCleaning\"]\n",
    "        \n",
    "    evyield = dict.fromkeys(cuts, 0)\n",
    "    for ik, pkl_file in enumerate(pkl_files):\n",
    "        with open(pkl_file, \"rb\") as f:\n",
    "            metadata = pkl.load(f)\n",
    "            \n",
    "        cutflows = metadata[sample][year][\"cutflows\"][ch]\n",
    "\n",
    "        for key in evyield.keys():\n",
    "\n",
    "            if key == \"sumgenweight\":\n",
    "                evyield[key] += metadata[sample][year][key] * xsec_weight\n",
    "            else:\n",
    "                evyield[key] += cutflows[key] * xsec_weight        \n",
    "    return evyield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a cut from the parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is your configuration. specefy which channels, years, samples, and directory of pkl files to use.\n",
    "channels = [\n",
    "    \"ele\", \n",
    "    \"mu\",\n",
    "]\n",
    "years = [\n",
    "#     \"2018\", \n",
    "    \"2017\",\n",
    "#     \"2016\", \n",
    "#     \"2016APV\",\n",
    "]\n",
    "\n",
    "samples = [\n",
    "    \"ggF\", \n",
    "    \"VBF\",  \n",
    "    \"WH\",\n",
    "    \"ZH\",    \n",
    "    \"ttH\",\n",
    "    \"WJetsLNu\",\n",
    "    \"TTbar\",\n",
    "    \"SingleTop\",\n",
    "    \"Diboson\",\n",
    "    \"EWKvjets\",\n",
    "    \"DYJets\",\n",
    "    \"WZQQ\",\n",
    "    \"Data\",\n",
    "]\n",
    "\n",
    "samples_dir = {\n",
    "#     \"2016\":    \"../eos/Oct10_hww_2016\",\n",
    "#     \"2016APV\": \"../eos/Oct10_hww_2016APV\",    \n",
    "#     \"2017\":    \"../eos/Oct10_hww_2017\",    \n",
    "#     \"2018\":    \"../eos/Oct10_hww_2018\",\n",
    "    \n",
    "    \"2016\":    \"../eos/hww/Dec20_hww_2016\",\n",
    "    \"2016APV\": \"../eos/hww/Dec20_hww_2016APV\",    \n",
    "    \"2017\":    \"../eos/hww/Dec20_hww_2017\",\n",
    "#     \"2017\":    \"../eos/hww/Jun5_hww_2017\",\n",
    "    \"2018\":    \"../eos/hww/Dec20_hww_2018\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2017\n",
      "  ele channel\n",
      "  mu channel\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cutflows = {}\n",
    "for year in years:\n",
    "#     if year != \"2016\":\n",
    "#         continue\n",
    "    print(f\"Processing year {year}\")\n",
    "    \n",
    "    cutflows[year] = {}\n",
    "    \n",
    "    for ch in channels:\n",
    "#         if ch != \"mu\": \n",
    "#             continue\n",
    "        \n",
    "        print(f\"  {ch} channel\")\n",
    "        cutflows[year][ch] = {}\n",
    "\n",
    "        condor_dir = os.listdir(samples_dir[year])\n",
    "\n",
    "        for sample in condor_dir:\n",
    "\n",
    "            # first: check if the sample is in one of combine_samples_by_name\n",
    "            sample_to_use = None\n",
    "            for key in utils.combine_samples_by_name:\n",
    "                if key in sample:\n",
    "                    sample_to_use = utils.combine_samples_by_name[key]\n",
    "                    break\n",
    "\n",
    "            # second: if not, combine under common label\n",
    "            if sample_to_use is None:\n",
    "                for key in utils.combine_samples:\n",
    "                    if key in sample:\n",
    "                        sample_to_use = utils.combine_samples[key]\n",
    "                        break\n",
    "                    else:\n",
    "                        sample_to_use = sample\n",
    "\n",
    "            if sample_to_use not in samples:\n",
    "                continue\n",
    "\n",
    "            is_data = False\n",
    "            if sample_to_use == \"Data\":\n",
    "                is_data = True\n",
    "\n",
    "            out_files = f\"{samples_dir[year]}/{sample}/outfiles/\"\n",
    "            pkl_files = glob.glob(f\"{out_files}/*.pkl\")\n",
    "\n",
    "            if len(pkl_files) == 0:\n",
    "                continue\n",
    "\n",
    "            parquet_files = glob.glob(f\"{out_files}/*_{ch}.parquet\")\n",
    "            \n",
    "            try:\n",
    "                data = pd.read_parquet(parquet_files)\n",
    "            except pyarrow.lib.ArrowInvalid:\n",
    "                # empty parquet because no event passed selection\n",
    "#                 print(f\"No parquet file for {sample}\")\n",
    "                continue\n",
    "\n",
    "            if len(data) == 0:\n",
    "#                 print(f\"Hi, No parquet file for {sample}\")\n",
    "                continue\n",
    "    \n",
    "            if sample_to_use not in cutflows[year][ch].keys():\n",
    "                cutflows[year][ch][sample_to_use] = get_cutflow(pkl_files, year, ch, sample, is_data)\n",
    "            else:\n",
    "                temp = get_cutflow(pkl_files, year, ch, sample, is_data)\n",
    "                for key in cutflows[year][ch][sample_to_use]:\n",
    "                    cutflows[year][ch][sample_to_use][key] += temp[key]\n",
    "            \n",
    "    print(f\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['WJetsLNu', 'EWKvjets', 'WH', 'TTbar', 'SingleTop', 'ggF', 'DYJets', 'Data', 'Diboson', 'WZQQ', 'ttH', 'VBF', 'ZH'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = cutflows[\"2017\"][\"ele\"].keys()  # samples\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing 2017 ele channel\n",
      "INFO:root:Processing 2017 mu channel\n"
     ]
    }
   ],
   "source": [
    "from make_stacked_hists import make_events_dict\n",
    "\n",
    "presel = {\n",
    "        \"mu\": {\n",
    "#             \"fj_mass\": \"fj_mass>40\",\n",
    "#             \"THWW>0.75\": \"THWW>0.750\",\n",
    "        },\n",
    "        \"ele\": {\n",
    "#             \"fj_mass\": \"fj_mass>40\",\n",
    "#             \"THWW>0.75\": \"THWW>0.750\",\n",
    "        },\n",
    "}\n",
    "\n",
    "THWW_path = \"../../weaver-core-dev/experiments_finetuning/v35_30/model.onnx\"\n",
    "\n",
    "events_dict = make_events_dict(years, channels, samples_dir, samples, presel, THWW_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the cut to the cutflow dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "presel = {\n",
    "        \"mu\": {\n",
    "            \"fj_mass\": \"fj_mass>40\",\n",
    "            \"THWW>0.75\": \"fj_mass>40 & THWW>0.75\",\n",
    "        },\n",
    "        \"ele\": {\n",
    "            \"fj_mass\": \"fj_mass>40\",\n",
    "            \"THWW>0.75\": \"fj_mass>40 & THWW>0.75\",\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in channels:\n",
    "    for cut, sel in list(presel[ch].items()):\n",
    "        for sample in samples:\n",
    "            for year in years:\n",
    "\n",
    "                df = events_dict[year][ch][sample]\n",
    "                df = df.query(sel)\n",
    "                \n",
    "                w = df[\"nominal\"]\n",
    "\n",
    "                cutflows[year][ch][sample][cut] = w.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumgenweight': 68395907.29737747,\n",
       " 'Trigger': 10561522.711528933,\n",
       " 'METFilters': 10555817.741991894,\n",
       " 'OneLep': 10519172.344996287,\n",
       " 'NoTaus': 8947622.779191008,\n",
       " 'AtLeastOneFatJet': 1028856.0673407008,\n",
       " 'CandidateJetpT': 467053.5136689998,\n",
       " 'LepInJet': 168386.5512486023,\n",
       " 'JetLepOverlap': 64711.53408324735,\n",
       " 'dPhiJetMET': 45520.409841926725,\n",
       " 'MET': 42561.39834191103,\n",
       " 'fj_mass': 48846.88393434201,\n",
       " 'THWW>0.75': 1352.2131225216735}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"2017\"][\"mu\"][\"WJetsLNu\"]     # take a quick look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumgenweight': 68397160.59608002,\n",
       " 'Trigger': 8685086.17951225,\n",
       " 'METFilters': 8680478.334096672,\n",
       " 'OneLep': 6799301.343566881,\n",
       " 'NoTaus': 6799301.343566881,\n",
       " 'AtLeastOneFatJet': 840620.0699936592,\n",
       " 'CandidateJetpT': 379483.6950056169,\n",
       " 'LepInJet': 148978.8602961507,\n",
       " 'JetLepOverlap': 50174.49584002223,\n",
       " 'dPhiJetMET': 34600.7690071595,\n",
       " 'MET': 32258.45996483975,\n",
       " 'fj_mass': 37709.64440129174,\n",
       " 'THWW>0.75': 970.0300195818509}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"2017\"][\"ele\"][\"WJetsLNu\"]     # take a quick look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumgenweight': 68395907.29737747,\n",
       " 'Trigger': 10561522.711528933,\n",
       " 'METFilters': 10555817.741991894,\n",
       " 'OneLep': 10519172.344996287,\n",
       " 'NoTaus': 8947622.779191008,\n",
       " 'AtLeastOneFatJet': 1028856.0673407008,\n",
       " 'CandidateJetpT': 467053.5136689998,\n",
       " 'LepInJet': 168386.5512486023,\n",
       " 'JetLepOverlap': 64711.53408324735,\n",
       " 'dPhiJetMET': 45520.409841926725,\n",
       " 'MET': 42561.39834191103,\n",
       " 'fj_mass': 48649.57380277161,\n",
       " 'THWW>0.75': 1347.2342165088326}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_cuts = cutflows[\"2018\"][\"mu\"][\"WJetsLNu\"]\n",
    "common_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_channels(cutflows):\n",
    "\n",
    "    # combine both channels\n",
    "    cutflows_new = {}\n",
    "    for year in cutflows.keys():\n",
    "        cutflows_new[year] = {}\n",
    "        cutflows_new[year][\"lep\"] = {}\n",
    "        \n",
    "        for ch in [\"mu\", \"ele\"]:\n",
    "            for sample in cutflows[year][ch]:\n",
    "                                \n",
    "                if sample not in cutflows_new[year][\"lep\"]:\n",
    "                    cutflows_new[year][\"lep\"][sample] = {}\n",
    "                \n",
    "                for cut in cutflows[year][ch][sample]:\n",
    "                    \n",
    "                    if (year != \"2018\") and (cut == \"HEMCleaning\"):\n",
    "                        continue\n",
    "                    \n",
    "                    if cut not in cutflows_new[year][\"lep\"][sample]:\n",
    "                        cutflows_new[year][\"lep\"][sample][cut] = cutflows[year][ch][sample][cut]\n",
    "                    else:\n",
    "                        cutflows_new[year][\"lep\"][sample][cut] += cutflows[year][ch][sample][cut]\n",
    "        cutflows[year] = {**cutflows[year], **cutflows_new[year]}\n",
    "        \n",
    "    return cutflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutflows = combine_channels(cutflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'2018'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcutflows\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2018\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mKeyError\u001b[0m: '2018'"
     ]
    }
   ],
   "source": [
    "cutflows[\"2018\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutflows[\"2018\"][\"ele\"][\"WJetsLNu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutflows[\"2018\"][\"mu\"][\"WJetsLNu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutflows[\"2018\"][\"lep\"][\"WJetsLNu\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_years(cutflows):\n",
    "    \"\"\"Will remove the HEM cleaning cutflow from 2018 first.\"\"\"\n",
    "    \n",
    "    whatever_year = list(cutflows.keys())[0]\n",
    "    channels = cutflows[whatever_year].keys()\n",
    "    \n",
    "    # combine all years\n",
    "    cutflows_new = {}\n",
    "    cutflows_new[\"Run2\"] = {}\n",
    "    \n",
    "    for ch in channels:\n",
    "        cutflows_new[\"Run2\"][ch] = {}\n",
    "        \n",
    "        for year in cutflows:\n",
    "            for sample in cutflows[year][ch]:\n",
    "                \n",
    "                if sample not in cutflows_new[\"Run2\"][ch]:\n",
    "                    cutflows_new[\"Run2\"][ch][sample] = {}\n",
    "\n",
    "                for cut in cutflows[year][ch][sample]:\n",
    "                    if \"HEM\" in cut:\n",
    "                        continue\n",
    "                    if cut not in cutflows_new[\"Run2\"][ch][sample]:\n",
    "                        cutflows_new[\"Run2\"][ch][sample][cut] = cutflows[year][ch][sample][cut]\n",
    "                    else:\n",
    "                        cutflows_new[\"Run2\"][ch][sample][cut] += cutflows[year][ch][sample][cut]\n",
    "\n",
    "    cutflows = {**cutflows, **cutflows_new}\n",
    "\n",
    "    return cutflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutflows = combine_years(cutflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['WJetsLNu', 'EWKvjets', 'WH', 'TTbar', 'SingleTop', 'ggF', 'DYJets', 'Data', 'Diboson', 'WZQQ', 'ttH', 'VBF', 'ZH'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"2017\"][\"ele\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2017', 'Run2'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ele', 'mu', 'lep'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"Run2\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine non-dominant backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine non-dominant backgrounds under others\n",
    "dominant_bkgs = [\"WJetsLNu\", \"TTbar\"]\n",
    "signals = [\"ggF\", \"VBF\", \"WH\", \"ZH\", \"ttH\"]\n",
    "\n",
    "for year in cutflows:\n",
    "    for ch in cutflows[year]:\n",
    "        cutflows[year][ch][\"Others\"] = dict.fromkeys(cutflows[year][ch][\"WJetsLNu\"], 0)\n",
    "        for sample in cutflows[year][ch]:\n",
    "            if sample == \"Data\":\n",
    "                continue\n",
    "            if sample not in signals+dominant_bkgs:\n",
    "                for cut in cutflows[year][ch][sample]:\n",
    "                    cutflows[year][ch][\"Others\"][cut] += cutflows[year][ch][sample][cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['WJetsLNu', 'EWKvjets', 'WH', 'TTbar', 'SingleTop', 'ggF', 'DYJets', 'Data', 'Diboson', 'WZQQ', 'ttH', 'VBF', 'ZH', 'Others'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"2017\"][\"ele\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumgenweight': 504923757.8414528,\n",
       " 'Trigger': 70230387.808001,\n",
       " 'METFilters': 70200411.81948991,\n",
       " 'OneLep': 46194136.59237416,\n",
       " 'NoTaus': 42957715.67864163,\n",
       " 'AtLeastOneFatJet': 1239221.4055518333,\n",
       " 'CandidateJetpT': 570697.1902661477,\n",
       " 'LepInJet': 280836.05979165446,\n",
       " 'JetLepOverlap': 106246.29007646533,\n",
       " 'dPhiJetMET': 63896.01737911475,\n",
       " 'MET': 55330.25217421119,\n",
       " 'fj_mass': 42631.21810229422,\n",
       " 'THWW>0.75': 753.6171800231882}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflows[\"2017\"][\"lep\"][\"Others\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LateX cutflow table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = [\n",
    "    \"sumgenweight\",\n",
    "    \"Trigger\",\n",
    "    \"METFilters\",\n",
    "    \"OneLep\",        \n",
    "    \"NoTaus\",\n",
    "    \"AtLeastOneFatJet\",\n",
    "    \"CandidateJetpT\",\n",
    "    \"LepInJet\",\n",
    "    \"JetLepOverlap\",\n",
    "    \"dPhiJetMET\",\n",
    "    \"MET\",\n",
    "    \"HEMCleaning\",\n",
    "]\n",
    "\n",
    "for cut in presel[\"mu\"]:\n",
    "    cuts += [cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_to_label = {\n",
    "    \"sumgenweight\": \"sumgenweight\",        \n",
    "    \"HEMCleaning\": \"HEMCleaning\",    \n",
    "    \"Trigger\": \"Trigger\",\n",
    "    \"METFilters\": \"METFilters\",\n",
    "    \"OneLep\": \"n Leptons = 1\",\n",
    "    \"NoTaus\": \"n Taus = 0\",\n",
    "    \"AtLeastOneFatJet\": r\"n FatJets $>=$ 1\",\n",
    "    \"CandidateJetpT\": r\"j $p_T > 250$GeV\",\n",
    "    \"LepInJet\": r\"$\\Delta R(j, \\ell) < 0.8$\",\n",
    "    \"JetLepOverlap\": r\"$\\Delta R(j, \\ell) > 0.03$\",\n",
    "    \"dPhiJetMET\": r\"$\\Delta \\phi(\\mathrm{MET}, j)<1.57$\",\n",
    "    \"MET\": r\"$\\mathrm{MET}>20$\",\n",
    "    \n",
    "    \"None\": \"None\",\n",
    "\n",
    "    \"fj_mass\": r\"j $\\mathrm{softdrop} > 40$GeV\",\n",
    "    \n",
    "    \"THWW>0.75\": r\"$\\ensuremath{T_{\\text{HWW}}^{\\ell\\nu qq}} > 0.75$\",\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_to_latex = {\n",
    "    \"WJetsLNu\": \"$\\PW(\\Pell\\PGn)$+\",\n",
    "    \"TTbar\": \"\\\\ttbar\",\n",
    "    \"Others\": \"Other MC\",\n",
    "\n",
    "    \"ggF\": \"ggF\",\n",
    "    \"VBF\": \"VBF\",\n",
    "    \"WH\": \"WH\",\n",
    "    \"ZH\": \"ZH\",    \n",
    "    \"ttH\": \"$t\\\\bar{t}H$\",    \n",
    "    \n",
    "    \"Data\": \"Data\",\n",
    "}\n",
    "\n",
    "def make_latex_cutflow_table(cutflows_dict, year, ch, add_data=False, add_sumgenweight=False):\n",
    "    \"\"\"Will use the cutflows dictionary to make the LateX table we have in the AN.\"\"\"\n",
    "    \n",
    "    samples_bkg = [\"WJetsLNu\", \"TTbar\", \"Others\"]\n",
    "    samples_sig = [\"ggF\",\"VBF\", \"WH\", \"ZH\", \"ttH\"]\n",
    "\n",
    "    ### backgrounds\n",
    "    headers = [parquet_to_latex[s] for s in samples_bkg]\n",
    "    \n",
    "    textabular = f\"l{'r'*len(headers)}\"\n",
    "    textabular += \"|r\"\n",
    "    \n",
    "    texheader = \"\\\\textbf{Inclusive Selection}\" + \" & \" + \" & \".join(headers) + \" & Total MC \"\n",
    "    if add_data:\n",
    "        textabular += \"|r\"\n",
    "        texheader += \"& Data \"\n",
    "    texheader += \"\\\\\\\\\"\n",
    "    texdata = \"\\\\hline\\n\"\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    for cut in cuts: \n",
    "        if (year != \"2018\") and (cut == \"HEMCleaning\"):\n",
    "            continue\n",
    "            \n",
    "        if not add_sumgenweight and cut == \"sumgenweight\":\n",
    "            continue\n",
    "    \n",
    "        data[cut] = []\n",
    "\n",
    "        for sample in samples_bkg:            \n",
    "            data[cut].append(round(cutflows_dict[year][ch][sample][cut]))\n",
    "            \n",
    "        totalmc = 0\n",
    "        for sample in (samples_bkg + samples_sig):\n",
    "            totalmc += round(cutflows_dict[year][ch][sample][cut])\n",
    "            \n",
    "        data[cut].append(totalmc)\n",
    "        \n",
    "        if add_data:\n",
    "            data[cut].append(round(cutflows_dict[year][ch][\"Data\"][cut]))\n",
    "\n",
    "    for label in data:\n",
    "        if label == \"z\":\n",
    "            texdata += \"\\\\hline\\n\"\n",
    "        texdata += f\"{cut_to_label[label]} & {' & '.join(map(str,data[label]))} \\\\\\\\\\n\"\n",
    "        \n",
    "    texdata += \"\\\\hline\\n\"    \n",
    "\n",
    "    ### signal\n",
    "    headers2 = [parquet_to_latex[s] for s in samples_sig]\n",
    "    texheader2 = \" & \" + \" & \".join(headers2) + \"\\\\\\\\\"\n",
    "    texdata2 = \"\\\\hline\\n\"\n",
    "\n",
    "    textabular2 = f\"l{'r'*len(headers2)}\"\n",
    "    \n",
    "    data = dict()\n",
    "    for cut in cuts:\n",
    "        if (year != \"2018\") and (cut == \"HEMCleaning\"):\n",
    "            continue\n",
    "            \n",
    "        data[cut] = []\n",
    "\n",
    "        for sample in samples_sig:\n",
    "            data[cut].append(round(cutflows_dict[year][ch][sample][cut]))\n",
    "        \n",
    "    for label in data:\n",
    "        if label == \"z\":\n",
    "            texdata += \"\\\\hline\\n\"\n",
    "        texdata2 += f\"{cut_to_label[label]} & {' & '.join(map(str,data[label]))} \\\\\\\\\\n\"    \n",
    "\n",
    "    # make table\n",
    "    print(\"\\\\begin{table}[!htp]\")\n",
    "    print(\"\\\\begin{center}\")\n",
    "    \n",
    "    print(\"\\\\begin{tabular}{\"+textabular+\"}\")\n",
    "    print(texheader)\n",
    "    print(texdata,end=\"\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "\n",
    "    print(\"\\\\begin{tabular}{\"+textabular2+\"}\")\n",
    "    print(texheader2)\n",
    "    print(texdata2,end=\"\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    \n",
    "    \n",
    "    if ch == \"lep\":\n",
    "        print(\"\\\\caption{Event yield of \" + year + \" Monte Carlo samples normalized to \" + str(round(get_lumi([year], [ch]))) + \"\\\\fbinv.}\")        \n",
    "    else:\n",
    "        print(\"\\\\caption{Event yield of \" + ch + \" channel \" + year + \" Monte Carlo samples normalized to \" + str(round(get_lumi([year], [ch]))) + \"\\\\fbinv.}\")\n",
    "        \n",
    "    print(\"\\\\label{sel-tab-cutflow\" + year + \"}\")\n",
    "    print(\"\\\\end{center}\")\n",
    "    print(\"\\\\end{table}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htp]\n",
      "\\begin{center}\n",
      "\\begin{tabular}{lrrr|r|r}\n",
      "\\textbf{Inclusive Selection} & $\\PW(\\Pell\\PGn)$+ & \\ttbar & Other MC & Total MC & Data \\\\\n",
      "\\hline\n",
      "sumgenweight & 136793068 & 68995474 & 504923758 & 711218464 & 1202391493 \\\\\n",
      "Trigger & 19246609 & 6978344 & 70230388 & 96493182 & 619791460 \\\\\n",
      "METFilters & 19236296 & 6973776 & 70200412 & 96448311 & 619630992 \\\\\n",
      "n Leptons = 1 & 17318474 & 5893155 & 46194137 & 69438252 & 442103860 \\\\\n",
      "n Taus = 0 & 15746924 & 4704050 & 42957716 & 63437606 & 424640213 \\\\\n",
      "n FatJets $>=$ 1 & 1869476 & 882238 & 1239221 & 3993345 & 5032388 \\\\\n",
      "j $p_T > 250$GeV & 846537 & 448262 & 570697 & 1866770 & 2000795 \\\\\n",
      "$\\Delta R(j, \\ell) < 0.8$ & 317365 & 198780 & 280836 & 797719 & 993318 \\\\\n",
      "$\\Delta R(j, \\ell) > 0.03$ & 114886 & 171166 & 106246 & 392979 & 456364 \\\\\n",
      "$\\Delta \\phi(\\mathrm{MET}, j)<1.57$ & 80121 & 107940 & 63896 & 252418 & 252747 \\\\\n",
      "$\\mathrm{MET}>20$ & 74820 & 101745 & 55330 & 232323 & 229103 \\\\\n",
      "j $\\mathrm{softdrop} > 40$GeV & 86557 & 81306 & 42631 & 210678 & 192491 \\\\\n",
      "$\\ensuremath{T_{\\text{HWW}}^{\\ell\\nu qq}} > 0.75$ & 2322 & 868 & 754 & 4005 & 3512 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\begin{tabular}{lrrrrr}\n",
      " & ggF & VBF & WH & ZH & $t\\bar{t}H$\\\\\n",
      "\\hline\n",
      "sumgenweight & 381489 & 67042 & 24335 & 15669 & 17629 \\\\\n",
      "Trigger & 27710 & 4034 & 2398 & 1207 & 2492 \\\\\n",
      "METFilters & 27704 & 4031 & 2397 & 1206 & 2489 \\\\\n",
      "n Leptons = 1 & 24288 & 3451 & 1952 & 897 & 1898 \\\\\n",
      "n Taus = 0 & 22393 & 2904 & 1570 & 737 & 1312 \\\\\n",
      "n FatJets $>=$ 1 & 1251 & 296 & 185 & 82 & 596 \\\\\n",
      "j $p_T > 250$GeV & 646 & 135 & 96 & 44 & 353 \\\\\n",
      "$\\Delta R(j, \\ell) < 0.8$ & 424 & 78 & 46 & 26 & 164 \\\\\n",
      "$\\Delta R(j, \\ell) > 0.03$ & 404 & 73 & 31 & 23 & 150 \\\\\n",
      "$\\Delta \\phi(\\mathrm{MET}, j)<1.57$ & 281 & 53 & 21 & 13 & 93 \\\\\n",
      "$\\mathrm{MET}>20$ & 260 & 49 & 19 & 12 & 88 \\\\\n",
      "j $\\mathrm{softdrop} > 40$GeV & 80 & 35 & 15 & 10 & 44 \\\\\n",
      "$\\ensuremath{T_{\\text{HWW}}^{\\ell\\nu qq}} > 0.75$ & 33 & 15 & 5 & 3 & 5 \\\\\n",
      "\\end{tabular}\n",
      "\\caption{Event yield of 2017 Monte Carlo samples normalized to 41\\fbinv.}\n",
      "\\label{sel-tab-cutflow2017}\n",
      "\\end{center}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "make_latex_cutflow_table(cutflows, \"2017\", \"lep\", add_data=True, add_sumgenweight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea-env",
   "language": "python",
   "name": "coffea-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
